{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873ff9ef",
   "metadata": {},
   "source": [
    "# Independently Training Machines/Agents (IPPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c076f71",
   "metadata": {},
   "source": [
    "Each Machine/Agent learns on its own (no collaboration, and they don't train together). Decentralized Training, Decentralized Execution.\n",
    "\n",
    "The PPO's are trained separately for each agent. During training, the chosen agent choses its own action while the others choose random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a96a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorboard\n",
    "# %pip install stable-baselines3\n",
    "# %pip install stable-baselines3[extra]\n",
    "# %pip install pytorch\n",
    "# %pip install gymnasium\n",
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2c3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent.parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "data_file = project_root / \"data\" / \"Input_JSON_Schedule_Optimization.json\"\n",
    "with open(data_file) as f:\n",
    "    data = json.load(f) # ensure that this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b398ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train independent agents (one per machine) using PPO.\n",
    "Each agent learns independently without knowing about other agents.\n",
    "This is the simplest MARL approach.\n",
    "\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "from backend.schedulers.marl.factory_gym_env import FactoryMultiAgentEnv\n",
    "from backend.factory_logic_loader import FactoryLogicLoader\n",
    "from backend.job_builder import JobBuilder\n",
    "from backend.schemas import ProductRequest\n",
    "\n",
    "\n",
    "class IndependentAgentWrapper(gym.Env):\n",
    "    \"\"\"\n",
    "    Wrapper that presents single-agent interface for one machine.\n",
    "    Used to train each agent independently with Stable-Baselines3.\n",
    "    \"\"\"\n",
    "    def __init__(self, multi_agent_env: FactoryMultiAgentEnv, agent_id: str):\n",
    "        self.env = multi_agent_env\n",
    "        self.agent_id = agent_id # this independent agent\n",
    "        self.observation_space = multi_agent_env.observation_spaces[agent_id]\n",
    "        self.action_space = multi_agent_env.action_spaces[agent_id]\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations, _ = self.env.reset(seed=seed, options=options)\n",
    "        return observations[self.agent_id], {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # all agents take random actions\n",
    "        actions = {\n",
    "            agent: self.env.action_spaces[agent].sample()\n",
    "            for agent in self.env.agents\n",
    "        }\n",
    "        actions[self.agent_id] = action  # Override this agent with learned action\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(actions)\n",
    "            \n",
    "        terminated = terminations[self.agent_id]\n",
    "        truncated = truncations[self.agent_id]\n",
    "        obs = observations[self.agent_id]\n",
    "        reward = rewards[self.agent_id]\n",
    "        info = infos[self.agent_id]\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def train_independent_agents(\n",
    "    factory_logic,\n",
    "    jobs,\n",
    "    total_timesteps=1152,\n",
    "    save_dir=\"models/marl_independent\"\n",
    "):\n",
    "    \"\"\"Train one agent per machine independently\"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create base environment\n",
    "    base_env = FactoryMultiAgentEnv(\n",
    "        factory_logic=factory_logic,\n",
    "        initial_jobs=jobs,\n",
    "        max_steps=1152\n",
    "    )\n",
    "    \n",
    "    agents = {}\n",
    "    \n",
    "    # Train each agent\n",
    "    for agent_id in base_env.possible_agents:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training agent: {agent_id}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create environment for this agent\n",
    "        env = FactoryMultiAgentEnv(\n",
    "            factory_logic=factory_logic,\n",
    "            initial_jobs=jobs,\n",
    "            max_steps=1152\n",
    "        )\n",
    "        single_agent_env = IndependentAgentWrapper(env, agent_id)\n",
    "        vec_env = DummyVecEnv([lambda: single_agent_env])\n",
    "        \n",
    "        # Create PPO agent\n",
    "        model = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            vec_env,\n",
    "            verbose=1,\n",
    "            learning_rate=3e-4,\n",
    "            n_steps=2048,\n",
    "            batch_size=64,\n",
    "            n_epochs=10,\n",
    "            gamma=0.99,\n",
    "            tensorboard_log=f\"{save_dir}/tensorboard/{agent_id}\"\n",
    "        )\n",
    "        \n",
    "        # Setup callbacks\n",
    "        checkpoint_callback = CheckpointCallback(\n",
    "            save_freq=10_000,\n",
    "            save_path=f\"{save_dir}/checkpoints/{agent_id}\",\n",
    "            name_prefix=f\"ppo_{agent_id}\"\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=checkpoint_callback,\n",
    "            progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Save final model\n",
    "        model.save(f\"{save_dir}/{agent_id}_final\")\n",
    "        print(f\"Saved model for {agent_id}\")\n",
    "        \n",
    "        agents[agent_id] = model\n",
    "    \n",
    "    return agents\n",
    "\n",
    "\n",
    "def evaluate_trained_agents(agents: Dict, factory_logic, jobs, num_episodes=10):\n",
    "    \"\"\"Evaluate the trained agents\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"EVALUATING TRAINED AGENTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_costs = []\n",
    "    episode_completion_rates = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        env = FactoryMultiAgentEnv(\n",
    "            factory_logic=factory_logic,\n",
    "            initial_jobs=jobs,\n",
    "            max_steps=1152\n",
    "        )\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        infos = None  # will hold last infos dict\n",
    "        \n",
    "        while not done:\n",
    "            # Each agent predicts its action\n",
    "            actions = {}\n",
    "            for agent_id in env.agents:\n",
    "                action, _ = agents[agent_id].predict(obs[agent_id], deterministic=True)\n",
    "                actions[agent_id] = action\n",
    "            \n",
    "            obs, rewards, terms, truncs, infos = env.step(actions)\n",
    "            \n",
    "            # average reward over agents for this step\n",
    "            episode_reward += sum(rewards.values()) / max(len(rewards), 1)\n",
    "            done = all(terms.values()) or all(truncs.values())\n",
    "        \n",
    "        # Episode finished -> compute completion from factory state\n",
    "        factory_state = env.factory.get_factory_state()\n",
    "        total_jobs = len(factory_state.jobs)\n",
    "        done_jobs = sum(1 for j in factory_state.jobs if j.done)\n",
    "        completion_rate = done_jobs / max(total_jobs, 1)\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_costs.append(env.total_episode_power_cost)\n",
    "        episode_completion_rates.append(completion_rate)\n",
    "        \n",
    "        print(\n",
    "            f\"Episode {episode + 1}: \"\n",
    "            f\"Reward={episode_reward:.2f}, \"\n",
    "            f\"Cost=${env.total_episode_power_cost:.2f}, \"\n",
    "            f\"Completion={completion_rate*100:.1f}%\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"RESULTS:\")\n",
    "    print(f\"  Avg Reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"  Avg Cost: ${np.mean(episode_costs):.2f} ± {np.std(episode_costs):.2f}\")\n",
    "    print(f\"  Avg Completion: {np.mean(episode_completion_rates)*100:.1f}%\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_agents(save_dir=\"models/marl_independent\"):\n",
    "    \"\"\"Load trained agents from directory\"\"\"\n",
    "    agents = {}\n",
    "    for filename in os.listdir(save_dir):\n",
    "        if not filename.endswith(\".zip\"):\n",
    "            continue\n",
    "\n",
    "        base = filename.split(\".\")[0]  # e.g. \"MAQ118_final\"\n",
    "        # Strip \"_final\" suffix so it matches env.agents (\"MAQ118\")\n",
    "        if base.endswith(\"_final\"):\n",
    "            agent_id = base[:-6]  # remove \"_final\"\n",
    "        else:\n",
    "            agent_id = base\n",
    "\n",
    "        agents[agent_id] = PPO.load(os.path.join(save_dir, filename))\n",
    "\n",
    "    return agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e6971",
   "metadata": {},
   "source": [
    "## Train IPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c99af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file) as f:\n",
    "    data = json.load(f) # frontend will change this, product_requests and steps specifically\n",
    "\n",
    "product_requests_data = data[\"product_requests\"] # frontend will change this, product_requests and steps specifically\n",
    "product_requests = [ProductRequest(**pr) for pr in product_requests_data]\n",
    "\n",
    "factory_logic = FactoryLogicLoader.load_from_file(filepath=data_file)\n",
    "job_builder = JobBuilder(factory_logic=factory_logic)\n",
    "jobs = job_builder.build_jobs(product_requests=product_requests) # job objects\n",
    "\n",
    "print(f\"Training with {len(jobs)} jobs\")\n",
    "print(f\"Machines: {list(factory_logic.machines.keys())}\")\n",
    "\n",
    "# Train agents\n",
    "agents = train_independent_agents(\n",
    "    factory_logic=factory_logic,\n",
    "    jobs=jobs,\n",
    "    total_timesteps=100_000,  # Start small, increase later\n",
    "    save_dir=\"models/marl_independent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e09c8",
   "metadata": {},
   "source": [
    "## Evaluate IPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d270a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATING TRAINED AGENTS\n",
      "==================================================\n",
      "Episode 1: Reward=1253.43, Cost=$0.00, Completion=99.2%\n",
      "\n",
      "==================================================\n",
      "RESULTS:\n",
      "  Avg Reward: 1253.43 ± 0.00\n",
      "  Avg Cost: $0.00 ± 0.00\n",
      "  Avg Completion: 99.2%\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(data_file) as f:\n",
    "    data = json.load(f) # frontend will change this, product_requests and steps specifically\n",
    "\n",
    "product_requests_data = data[\"product_requests\"] # frontend will change this, product_requests and steps specifically\n",
    "product_requests = [ProductRequest(**pr) for pr in product_requests_data]\n",
    "\n",
    "factory_logic = FactoryLogicLoader.load_from_file(filepath=data_file)\n",
    "job_builder = JobBuilder(factory_logic=factory_logic)\n",
    "jobs = job_builder.build_jobs(product_requests=product_requests) # job objects\n",
    "\n",
    "\n",
    "# load agents\n",
    "agents = load_agents(save_dir=\"models/marl_independent\")\n",
    "\n",
    "# Evaluate\n",
    "evaluate_trained_agents(agents, factory_logic, jobs, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eee1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
